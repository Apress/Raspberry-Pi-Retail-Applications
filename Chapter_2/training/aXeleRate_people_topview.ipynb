{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "aXeleRate_people_topview.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Topview Camera Person Detection model Training and Inference\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AIWintermuteAI/Raspberry_Pi_Retail_Applications/blob/main/Chapter_2/training/aXeleRate_people_topview.ipynb)\n",
        "\n",
        "In this notebook we will use axelerate, Keras-based framework for AI on the edge, to quickly setup model training and then after training session is completed convert it to .tflite and .kmodel formats.\n",
        "\n",
        "First, let's take care of some administrative details. \n",
        "\n",
        "1) Before we do anything, make sure you have choosen GPU as Runtime type (in Runtime - > Change Runtime type).\n",
        "\n",
        "2) We need to mount Google Drive for saving our model checkpoints and final converted model(s). Press on Mount Google Drive button in Files tab on your left. \n",
        "\n",
        "In the next cell we clone axelerate Github repository and import it. \n",
        "\n",
        "**It is possible to use pip install or python setup.py install, but in that case you will need to restart the enironment.** Since I'm trying to make the process as streamlined as possibile I'm using sys.path.append for import."
      ],
      "metadata": {
        "id": "hS9yMrWe02WQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#we need imgaug 0.4 for image augmentations to work properly, see https://stackoverflow.com/questions/62580797/in-colab-doing-image-data-augmentation-with-imgaug-is-not-working-as-intended\n",
        "!pip uninstall -y imgaug && pip uninstall -y albumentations && pip install imgaug==0.4\n",
        "!git clone https://github.com/AIWintermuteAI/aXeleRate.git\n",
        "import sys\n",
        "sys.path.append('/content/aXeleRate')\n",
        "from axelerate import setup_training,setup_inference"
      ],
      "outputs": [],
      "metadata": {
        "id": "y07yAbYbjV2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this step you typically need to get the dataset. You can use !wget command to download it from somewhere on the Internet or !cp to copy from My Drive as in this example\n",
        "```\n",
        "!cp -r /content/drive/'My Drive'/pascal_20_segmentation.zip .\n",
        "!unzip --qq pascal_20_segmentation.zip\n",
        "```\n",
        "For this notebook we will use dataset, that is comprised of data from primarily three sources:\n",
        "- synthetic images generated with Nvidia Isaac SDK\n",
        "- images converted from PIROPO Database videos\n",
        "- personal recordings of author converted to images\n",
        "If youâ€™d like to make your own dataset or add some samples to existing one, you can use any object detection dataset annotation tool available as long as it supports export to Pascal VOC format.\n",
        "\n",
        "I split the dataset into training and validation using a simple Python script. Since most of the models trained with aXeleRate are to be run on embedded devices and thus have memory and latency constraints, the validation images are easier than most of the images in training set. The validation images include one(or many) instance of a particular class, no mixed classes in one image.\n",
        "\n",
        "Let's visualize our detection model test dataset. We use img_num=10 to show only first 10 images. Feel free to change the number to None to see all 100 images.\n"
      ],
      "metadata": {
        "id": "5TBRMPZ83dRL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%matplotlib inline\n",
        "!gdown https://drive.google.com/uc?id=1C96zauDhD5qlJdz76kX9vJXsZgC7H-01  #pascal-voc dataset\n",
        "!gdown https://drive.google.com/uc?id=1PzHP4V30YWYcdVce995DSCVh8rE7yomQ #pre-trained model\n",
        "!unzip --qq people_topview.zip\n",
        "\n",
        "from axelerate.networks.common_utils.augment import visualize_detection_dataset\n",
        "\n",
        "visualize_detection_dataset(img_folder='people_topview/imgs_validation', ann_folder='people_topview/anns_validation', num_imgs=10, img_size=224, augment=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "_tpsgkGj7d79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step is defining a config dictionary. Most lines are self-explanatory.\n",
        "\n",
        "Type is model frontend - Classifier, Detector or Segnet\n",
        "\n",
        "Architecture is model backend (feature extractor) \n",
        "\n",
        "- Full Yolo\n",
        "- Tiny Yolo\n",
        "- MobileNet1_0\n",
        "- MobileNet7_5 \n",
        "- MobileNet5_0 \n",
        "- MobileNet2_5 \n",
        "- SqueezeNet\n",
        "- NASNetMobile\n",
        "- DenseNet121\n",
        "- ResNet50\n",
        "\n",
        "For more information on anchors, please read here\n",
        "https://github.com/pjreddie/darknet/issues/568\n",
        "\n",
        "Labels are labels present in your dataset.\n",
        "IMPORTANT: Please, list all the labels present in the dataset.\n",
        "\n",
        "object_scale determines how much to penalize wrong prediction of confidence of object predictors\n",
        "\n",
        "no_object_scale determines how much to penalize wrong prediction of confidence of non-object predictors\n",
        "\n",
        "coord_scale determines how much to penalize wrong position and size predictions (x, y, w, h)\n",
        "\n",
        "class_scale determines how much to penalize wrong class prediction\n",
        "\n",
        "For converter type you can choose the following:\n",
        "\n",
        "'k210', 'tflite_fullint', 'tflite_dynamic', 'edgetpu', 'openvino', 'onnx'\n",
        "\n",
        "**You can set weights-full field to the pre-trained model if you want to do transfer learning** "
      ],
      "metadata": {
        "id": "S1oqdtbr7VLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "config = {\n",
        "        \"model\":{\n",
        "            \"type\":                 \"Detector\",\n",
        "            \"architecture\":         \"MobileNet7_5\",\n",
        "            \"input_size\":           [224, 224],\n",
        "            \"anchors\":              [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828],\n",
        "            \"labels\":               [\"person\"],\n",
        "            \"coord_scale\" : \t\t1.0,\n",
        "            \"class_scale\" : \t\t1.0,\n",
        "            \"object_scale\" : \t\t5.0,\n",
        "            \"no_object_scale\" : \t1.0\n",
        "        },\n",
        "        \"weights\" : {\n",
        "            \"full\":   \t\t\t\t  \"\",\n",
        "            \"backend\":   \t\t    \"imagenet\"\n",
        "        },\n",
        "        \"train\" : {\n",
        "            \"actual_epoch\":         50,\n",
        "            \"train_image_folder\":   \"people_topview/imgs\",\n",
        "            \"train_annot_folder\":   \"people_topview/anns\",\n",
        "            \"train_times\":          1,\n",
        "            \"valid_image_folder\":   \"people_topview/imgs_validation\",\n",
        "            \"valid_annot_folder\":   \"people_topview/anns_validation\",\n",
        "            \"valid_times\":          1,\n",
        "            \"valid_metric\":         \"mAP\",\n",
        "            \"batch_size\":           32,\n",
        "            \"learning_rate\":        1e-3,\n",
        "            \"saved_folder\":   \t\tF\"/content/drive/MyDrive/people_topview\",\n",
        "            \"first_trainable_layer\": \"\",\n",
        "            \"augumentation\":\t\t\t\tTrue,\n",
        "            \"is_only_detect\" : \t\tFalse\n",
        "        },\n",
        "        \"converter\" : {\n",
        "            \"type\":   \t\t\t\t[\"tflite_fullint\"]\n",
        "        }\n",
        "    }"
      ],
      "outputs": [],
      "metadata": {
        "id": "Jw4q6_MsegD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check what GPU we have been assigned in this Colab session, if any."
      ],
      "metadata": {
        "id": "kobC_7gd5mEu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "outputs": [],
      "metadata": {
        "id": "rESho_T70BWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we start the training by passing config dictionary we have defined earlier to setup_training function. The function will start the training with  Reduce Learning Rate on Plateau and save on best mAP callbacks. Every epoch mAP of the model predictions is measured on the validation dataset. If you have specified the converter type in the config, after the training has stopped the script will convert the best model into the format you have specified in config and save it to the project folder.\n",
        "\n",
        "Let's train for one epoch to see how the whole pipeline works."
      ],
      "metadata": {
        "id": "cWyKjw-b5_yp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from keras import backend as K \n",
        "K.clear_session()\n",
        "model_path = setup_training(config_dict=config)"
      ],
      "outputs": [],
      "metadata": {
        "id": "deYD3cwukHsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training it is good to check the actual perfomance of your model by doing inference on your validation dataset and visualizing results. This is exactly what next block does. Our model used pre-trained weights and since all the layers were set as non-trainable, we are just observing the perfomance of the model that was trained before."
      ],
      "metadata": {
        "id": "ypTe3GZI619O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%matplotlib inline\n",
        "from keras import backend as K \n",
        "K.clear_session()\n",
        "setup_inference(config, model_path)"
      ],
      "outputs": [],
      "metadata": {
        "id": "jE7pTYmZN7Pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good luck and happy training! Have a look at these articles, that would allow you to get the most of Google Colab or connect to local runtime if there are no GPUs available;\n",
        "\n",
        "https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403\n",
        "\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ],
      "metadata": {
        "id": "5YuVe2VD11cd"
      }
    }
  ]
}